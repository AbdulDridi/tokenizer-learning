{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# BPE Tokenizer\n",
    "\n",
    "This is a widely used two-stage algorithm introduced to the LLM domain by the [GPT-2 Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). The first builds a vocabulary of frequent subword units. Once this is built, the vocabulary is used to encode/decode text into/from tokens.\n",
    "\n",
    "## Step 1 - Building the Vocabulary\n",
    "\n",
    "The first step is to build a vocabulary of subword units from training data. The algorithm works by iteratively merging the most frequent pairs of characters or subwords until a predefined vocabulary size is reached. There's a \"sweet spot\" for the vocabulary size in the LLM domain:\n",
    "- One the one hand, a larger vocabulary allows for more compressed / accurate representation of words.\n",
    "- On the other hand, this bloats the LLM vocabulary, meaning the embeddings layer and output layers will be larger, also increasing the complexity of the final softmax operation.\n",
    "\n",
    "So picking the right vocabulary size is a balancing act. It's also important that the training data is representative of the language and domain the model will be used in, as this will affect the quality of the vocabulary, and thus will impact the downstream representational ability of the model."
   ],
   "id": "dfd53e96edcbb1e6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "VOCABULARY_SIZE = 100000\n",
    "\n",
    "# TODO: Update with better training data\n",
    "with open(\"../data/shakespeare.txt\", \"r\") as f:\n",
    "    TRAINING_TEXT = f.read()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note - I'll be performing tokenization at the byte level, the alternative is to tokenize at the Unicode Code Point level.\n",
    "\n",
    "Want to know more on text representation at this level? Check out [this article](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/) by Joel Spolsky.\n",
    "\n",
    "To keep things simple, UTF-8 encoding represents each character as a sequence of up to 4 bytes, where each byte is an integer between 0 and 255. This means, with the default vocabulary size of 100,000, we can represent the first 256 tokens as single byte tokens (0-255), and the rest as merged tokens."
   ],
   "id": "7a0903ad57013c0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_bytes = TRAINING_TEXT.encode('utf-8')\n",
    "\n",
    "next_token_id = 256 # bytes 0-255 are reserved for single byte tokens"
   ],
   "id": "ba2923a91bc46486",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In order to create a vocabulary, we need to count the frequency of each pair of tokens in the training data. We'll use a doubly linked list to represent the sequence of tokens, allowing us to efficiently merge pairs of tokens as we build the vocabulary. We'll also use a max heap to efficiently retrieve the most frequent pairs of tokens.",
   "id": "434dd0b9f0a8f424"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TokenNode:\n",
    "    def __init__(self, value):\n",
    "        self.value = value  # byte / merged token ID\n",
    "        self.prev = None\n",
    "        self.next = None\n",
    "\n",
    "def build_linked_list(token_list):\n",
    "    nodes = [TokenNode(tok) for tok in token_list]\n",
    "    for i in range(1, len(nodes)):\n",
    "        nodes[i].prev = nodes[i - 1]\n",
    "        nodes[i - 1].next = nodes[i]\n",
    "    return nodes\n"
   ],
   "id": "909f7a7c373cc5ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "doubly_linked_tokens = build_linked_list(training_bytes)",
   "id": "852953f3c7df403e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "pair_freqs = defaultdict(int)\n",
    "pair_positions = defaultdict(set)\n",
    "\n",
    "def index_pairs(nodes):\n",
    "    for node in nodes:\n",
    "        if node.next:\n",
    "            pair = (node.value, node.next.value)\n",
    "            pair_freqs[pair] += 1\n",
    "            pair_positions[pair].add(node)\n"
   ],
   "id": "63a5367357b64ec1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "index_pairs(doubly_linked_tokens)",
   "id": "c4f79c871e9a325a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def merge_pair(pair, new_token_id, max_heap):\n",
    "    nodes_to_merge = list(pair_positions[pair])\n",
    "    updated_pairs = set()\n",
    "    for node in nodes_to_merge:\n",
    "        if not node.next or (node.value, node.next.value) != pair:\n",
    "            continue\n",
    "\n",
    "        right = node.next\n",
    "\n",
    "        # Unlink the right node\n",
    "        node.value = new_token_id\n",
    "        node.next = right.next\n",
    "        if right.next:\n",
    "            right.next.prev = node\n",
    "\n",
    "        # Clean up stale pair positions\n",
    "        if node.prev:\n",
    "            old_left = (node.prev.value, pair[0])\n",
    "            pair_freqs[old_left] -= 1\n",
    "            pair_positions[old_left].discard(node.prev)\n",
    "            new_left = (node.prev.value, new_token_id)\n",
    "            pair_freqs[new_left] += 1\n",
    "            pair_positions[new_left].add(node.prev)\n",
    "            updated_pairs.add(new_left)\n",
    "\n",
    "        if node.next:\n",
    "            old_right = (pair[1], node.next.value)\n",
    "            pair_freqs[old_right] -= 1\n",
    "            pair_positions[old_right].discard(right)\n",
    "            new_right = (new_token_id, node.next.value)\n",
    "            pair_freqs[new_right] += 1\n",
    "            pair_positions[new_right].add(node)\n",
    "            updated_pairs.add(new_right)\n",
    "\n",
    "        # Clean up merged pair\n",
    "        pair_freqs[pair] -= 1\n",
    "        pair_positions[pair].discard(node)\n",
    "\n",
    "        # Add new/updated pairs to updated_pairs set, for adding to the heap later\n",
    "        updated_pairs.add(pair)\n",
    "\n",
    "    print(f\"Merged {pair} into {new_token_id} with {len(nodes_to_merge)} occurrences.\")\n",
    "    # Add updated pairs to the heap\n",
    "    for updated_pair in updated_pairs:\n",
    "        if pair_freqs[updated_pair] > 0:\n",
    "            heapq.heappush(max_heap, (-pair_freqs[updated_pair], updated_pair))"
   ],
   "id": "10b59bd0eb3997a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import heapq\n",
    "\n",
    "heap = [(-freq, pair) for pair, freq in pair_freqs.items()]\n",
    "heapq.heapify(heap)\n",
    "merge_list = []\n",
    "merge_table = {}\n",
    "while heap and len(merge_table) < VOCABULARY_SIZE - 256:\n",
    "    neg_freq, pair = heapq.heappop(heap)\n",
    "    if pair_freqs.get(pair, 0) != -neg_freq:\n",
    "        continue  # stale\n",
    "    new_token_id = next_token_id\n",
    "    merge_pair(pair, new_token_id, heap)\n",
    "    merge_table[pair] =  new_token_id\n",
    "    merge_list.append((pair, new_token_id))\n",
    "    next_token_id += 1\n"
   ],
   "id": "35a626b3301e9434",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merge_list:\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]"
   ],
   "id": "17bcaab285d311a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 2 - Encoding and Decoding\n",
    "\n",
    "Now that we've built a vocabulary, we can use it to encode and decode text by iterating over the merge rules and replacing pairs of tokens with their merged token ID. The encoding process will convert text into a sequence of token IDs, while the decoding process will convert token IDs back into text."
   ],
   "id": "72ca1a62068d8492"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Update Naive encoding method to use linked list & prio queues for O(nlog n) time complexity, instead of current O(m*n) where m is the number of merge rules and n is the length of the text.\n",
    "\n",
    "def encode(text, merge_list):\n",
    "    text_bytes = list(text.encode('utf-8'))\n",
    "    for merge_rule in merge_list:\n",
    "        pair, new_token = merge_rule\n",
    "        i = 0\n",
    "        while i < len(text_bytes) - 1 and len(text_bytes) > 1:\n",
    "            if (text_bytes[i], text_bytes[i + 1]) == pair:\n",
    "                text_bytes[i] = new_token\n",
    "                del text_bytes[i + 1]\n",
    "            i+= 1\n",
    "\n",
    "    return text_bytes\n",
    "\n",
    "\n",
    "def decode(ids):\n",
    "    # given ids (list of integers), return Python string\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n"
   ],
   "id": "dcaf5009f4b0657d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T16:03:28.181754Z",
     "start_time": "2025-07-01T16:03:27.999433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_string = \"how is this encoding method?\"\n",
    "encoded_ids = encode(input_string, merge_list)\n",
    "for idx in encoded_ids:\n",
    "    print(f\"token ID: {idx}, decoded individual token: {decode([idx])}\")\n",
    "\n",
    "print(f\"Decoded string: {decode(encoded_ids)}\")"
   ],
   "id": "719dbae668f3a640",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ID: 10986, decoded individual token: how is \n",
      "token ID: 370, decoded individual token: this \n",
      "token ID: 270, decoded individual token: en\n",
      "token ID: 99, decoded individual token: c\n",
      "token ID: 12178, decoded individual token: oding \n",
      "token ID: 109, decoded individual token: m\n",
      "token ID: 101, decoded individual token: e\n",
      "token ID: 257, decoded individual token: th\n",
      "token ID: 111, decoded individual token: o\n",
      "token ID: 100, decoded individual token: d\n",
      "token ID: 63, decoded individual token: ?\n",
      "Decoded string: how is this encoding method?\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7795326a7ad1ba8a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
